{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6128a66c",
   "metadata": {},
   "source": [
    "# Enhanced EDA & Preprocessing Pipeline for Diabetes Dataset\n",
    "\n",
    "This notebook is a direct conversion of `enhanced_eda_preprocess.py` into notebook form.\n",
    "Sections: imports, utility functions, enhanced EDA, visualization generation, outlier handling,\n",
    "enhanced imputation, feature engineering, high-cardinality handling, full preprocessing pipeline, and a `main` runner cell.\n",
    "\n",
    "Use the cells below interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81077a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced EDA & Preprocessing Pipeline for Diabetes Dataset\n",
    "# Addresses gaps: year column handling, feature engineering, outlier detection,\n",
    "# advanced imputation, comprehensive EDA, and high-dimensionality issues\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib backend for compatibility (non-interactive)\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fdf10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Utility Functions ----------\n",
    "def ensure_dir(p: str):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def is_binary_like(s: pd.Series) -> bool:\n",
    "    \"\"\"Check if series contains binary-like values\"\"\"\n",
    "    vals = s.dropna().unique()\n",
    "    if len(vals) == 2:\n",
    "        return True\n",
    "    lowered = pd.Series(vals).astype(str).str.lower().unique()\n",
    "    return set(lowered).issubset({\"yes\",\"no\",\"true\",\"false\",\"positive\",\"negative\",\"pos\",\"neg\",\"y\",\"n\",\"1\",\"0\"})\n",
    "\n",
    "def guess_target(df: pd.DataFrame):\n",
    "    \"\"\"Automatically detect target column\"\"\"\n",
    "    common = [\n",
    "        \"Outcome\",\"outcome\",\"target\",\"Target\",\"label\",\"Label\",\"class\",\"Class\",\n",
    "        \"diabetes\",\"Diabetes\",\"has_diabetes\",\"diabetic\",\"Diabetic\"\n",
    "    ]\n",
    "    for c in common:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "952d66b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Enhanced EDA Functions ----------\n",
    "def comprehensive_eda(df: pd.DataFrame, reports_dir: str, target_col: str = None):\n",
    "    \"\"\"Enhanced EDA with comprehensive analysis\"\"\"\n",
    "    ensure_dir(reports_dir)\n",
    "    \n",
    "    print(\"🔍 Running Comprehensive EDA...\")\n",
    "    \n",
    "    # 1. Basic Dataset Info\n",
    "    basic_info = {\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "        'duplicated_rows': df.duplicated().sum()\n",
    "    }\n",
    "    \n",
    "    # 2. Column types and info\n",
    "    dtypes_df = df.dtypes.astype(str).rename(\"dtype\").reset_index().rename(columns={\"index\":\"column\"})\n",
    "    dtypes_df['unique_values'] = [df[col].nunique() for col in df.columns]\n",
    "    dtypes_df['null_count'] = [df[col].isnull().sum() for col in df.columns]\n",
    "    dtypes_df['null_percentage'] = dtypes_df['null_count'].apply(lambda x: round(x / len(df) * 100, 2))\n",
    "    dtypes_df.to_csv(os.path.join(reports_dir, \"01_enhanced_dtypes.csv\"), index=False)\n",
    "    \n",
    "    # 3. Missing values analysis\n",
    "    miss_analysis = df.isnull().sum().reset_index()\n",
    "    miss_analysis.columns = ['column', 'missing_count']\n",
    "    miss_analysis['missing_pct'] = miss_analysis['missing_count'].apply(lambda x: round(x / len(df) * 100, 2))\n",
    "    miss_analysis = miss_analysis.sort_values('missing_pct', ascending=False)\n",
    "    miss_analysis.to_csv(os.path.join(reports_dir, \"02_enhanced_missing_values.csv\"), index=False)\n",
    "    \n",
    "    # 4. Identify column types\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Remove target from feature lists if specified\n",
    "    if target_col and target_col in numeric_cols:\n",
    "        numeric_cols.remove(target_col)\n",
    "    if target_col and target_col in categorical_cols:\n",
    "        categorical_cols.remove(target_col)\n",
    "    \n",
    "    # 5. Enhanced numeric analysis\n",
    "    if numeric_cols:\n",
    "        numeric_stats = df[numeric_cols].describe()\n",
    "        \n",
    "        # Add additional statistics\n",
    "        numeric_enhanced = numeric_stats.copy()\n",
    "        for col in numeric_cols:\n",
    "            data = df[col].dropna()\n",
    "            numeric_enhanced.loc['skewness', col] = stats.skew(data)\n",
    "            numeric_enhanced.loc['kurtosis', col] = stats.kurtosis(data)\n",
    "            numeric_enhanced.loc['cv', col] = data.std() / data.mean() if data.mean() != 0 else 0\n",
    "        \n",
    "        numeric_enhanced.round(4).to_csv(os.path.join(reports_dir, \"03_enhanced_numeric_analysis.csv\"))\n",
    "    \n",
    "    # 6. Categorical analysis\n",
    "    if categorical_cols:\n",
    "        cat_analysis = []\n",
    "        for col in categorical_cols:\n",
    "            unique_vals = df[col].nunique()\n",
    "            top_category = df[col].mode()[0] if not df[col].mode().empty else 'No Mode'\n",
    "            top_frequency = df[col].value_counts().iloc[0] if unique_vals > 0 else 0\n",
    "            \n",
    "            cat_analysis.append({\n",
    "                'column': col,\n",
    "                'unique_categories': unique_vals,\n",
    "                'top_category': top_category,\n",
    "                'top_frequency': top_frequency,\n",
    "                'top_percentage': round(top_frequency / len(df) * 100, 2)\n",
    "            })\n",
    "        \n",
    "        cat_df = pd.DataFrame(cat_analysis)\n",
    "        cat_df.to_csv(os.path.join(reports_dir, \"04_categorical_analysis.csv\"), index=False)\n",
    "    \n",
    "    # 7. Target distribution (if target specified)\n",
    "    if target_col and target_col in df.columns:\n",
    "        target_dist = df[target_col].value_counts().reset_index()\n",
    "        target_dist.columns = [target_col, 'count']\n",
    "        target_dist['percentage'] = target_dist['count'].apply(lambda x: round(x / len(df) * 100, 2))\n",
    "        target_dist.to_csv(os.path.join(reports_dir, \"05_target_distribution.csv\"), index=False)\n",
    "    \n",
    "    # 8. Outlier detection for numeric columns\n",
    "    outlier_analysis = []\n",
    "    for col in numeric_cols:\n",
    "        data = df[col].dropna()\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "        \n",
    "        outlier_analysis.append({\n",
    "            'column': col,\n",
    "            'outlier_count': len(outliers),\n",
    "            'outlier_percentage': round(len(outliers) / len(data) * 100, 2),\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound\n",
    "        })\n",
    "    \n",
    "    outlier_df = pd.DataFrame(outlier_analysis)\n",
    "    outlier_df.to_csv(os.path.join(reports_dir, \"06_outlier_analysis.csv\"), index=False)\n",
    "    \n",
    "    # 9. Correlation analysis (numeric columns only)\n",
    "    if len(numeric_cols) > 1:\n",
    "        corr_matrix = df[numeric_cols].corr()\n",
    "        \n",
    "        # Save correlation matrix\n",
    "        corr_matrix.round(3).to_csv(os.path.join(reports_dir, \"07_correlation_matrix.csv\"))\n",
    "        \n",
    "        # Create correlation heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                    square=True, linewidths=0.5)\n",
    "        plt.title('Feature Correlation Heatmap')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(reports_dir, \"correlation_heatmap.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 10. Generate comprehensive visualizations\n",
    "    generate_enhanced_visualizations(df, reports_dir, numeric_cols, categorical_cols, target_col)\n",
    "    \n",
    "    print(f\"✅ Enhanced EDA completed. Reports saved to: {reports_dir}\")\n",
    "    return basic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8873058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_enhanced_visualizations(df, reports_dir, numeric_cols, categorical_cols, target_col):\n",
    "    \"\"\"Generate comprehensive visualizations for all columns\"\"\"\n",
    "    \n",
    "    # Create visualizations subdirectory\n",
    "    viz_dir = os.path.join(reports_dir, \"visualizations\")\n",
    "    ensure_dir(viz_dir)\n",
    "    \n",
    "    # 1. Numeric columns - Histograms and Box plots\n",
    "    for col in numeric_cols:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Histogram\n",
    "        df[col].hist(bins=50, ax=axes[0], alpha=0.7, edgecolor='black')\n",
    "        axes[0].set_title(f'Histogram: {col}')\n",
    "        axes[0].set_xlabel(col)\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        \n",
    "        # Box plot\n",
    "        df.boxplot(column=col, ax=axes[1])\n",
    "        axes[1].set_title(f'Box Plot: {col}')\n",
    "        axes[1].set_ylabel(col)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, f\"numeric_{col}.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 2. Categorical columns - Bar plots\n",
    "    for col in categorical_cols:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Get value counts\n",
    "        value_counts = df[col].value_counts()\n",
    "        \n",
    "        # Limit to top 20 categories if too many\n",
    "        if len(value_counts) > 20:\n",
    "            value_counts = value_counts.head(20)\n",
    "            title_suffix = \" (Top 20)\"\n",
    "        else:\n",
    "            title_suffix = \"\"\n",
    "        \n",
    "        # Create bar plot\n",
    "        ax = value_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "        plt.title(f'Distribution: {col}{title_suffix}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, v in enumerate(value_counts.values):\n",
    "            ax.text(i, v + max(value_counts.values) * 0.01, str(v), \n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, f\"categorical_{col}.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 3. Target vs Features analysis (if target specified)\n",
    "    if target_col and target_col in df.columns:\n",
    "        target_viz_dir = os.path.join(viz_dir, \"target_analysis\")\n",
    "        ensure_dir(target_viz_dir)\n",
    "        \n",
    "        # Numeric features vs target\n",
    "        for col in numeric_cols:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # Create subplots\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            # Box plot by target\n",
    "            df.boxplot(column=col, by=target_col, ax=axes[0])\n",
    "            axes[0].set_title(f'{col} by {target_col}')\n",
    "            \n",
    "            # Histogram by target\n",
    "            for target_val in df[target_col].unique():\n",
    "                subset = df[df[target_col] == target_val][col]\n",
    "                axes[1].hist(subset, alpha=0.7, label=f'{target_col}={target_val}', bins=30)\n",
    "            \n",
    "            axes[1].set_title(f'{col} Distribution by {target_col}')\n",
    "            axes[1].set_xlabel(col)\n",
    "            axes[1].set_ylabel('Frequency')\n",
    "            axes[1].legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(target_viz_dir, f\"target_vs_{col}.png\"), dpi=300, bbox_inches='tight')\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e2b7dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Enhanced Preprocessing Functions ----------\n",
    "def detect_outliers_iqr(series, multiplier=1.5):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "def handle_outliers(df, numeric_cols, method='cap', multiplier=1.5):\n",
    "    \"\"\"Handle outliers in numeric columns\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    outlier_info = {}\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        outliers = detect_outliers_iqr(df_clean[col], multiplier)\n",
    "        outlier_count = outliers.sum()\n",
    "        \n",
    "        if outlier_count > 0:\n",
    "            if method == 'cap':\n",
    "                # Cap outliers\n",
    "                Q1 = df_clean[col].quantile(0.25)\n",
    "                Q3 = df_clean[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - multiplier * IQR\n",
    "                upper_bound = Q3 + multiplier * IQR\n",
    "                \n",
    "                df_clean[col] = np.where(df_clean[col] < lower_bound, lower_bound, df_clean[col])\n",
    "                df_clean[col] = np.where(df_clean[col] > upper_bound, upper_bound, df_clean[col])\n",
    "                \n",
    "            elif method == 'remove':\n",
    "                # Remove outliers (not recommended for large datasets)\n",
    "                df_clean = df_clean[~outliers]\n",
    "        \n",
    "        outlier_info[col] = {\n",
    "            'outlier_count': outlier_count,\n",
    "            'outlier_percentage': round(outlier_count / len(df) * 100, 2),\n",
    "            'method_applied': method if outlier_count > 0 else 'none'\n",
    "        }\n",
    "    \n",
    "    return df_clean, outlier_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b77e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_imputation(df, numeric_cols, categorical_cols, target_col=None):\n",
    "    \"\"\"Enhanced imputation strategies for different column types\"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    imputation_info = {}\n",
    "    \n",
    "    # Medical/Health-specific imputation logic\n",
    "    medical_features = ['bmi', 'hbA1c_level', 'blood_glucose_level', 'sleep_hours']\n",
    "    \n",
    "    # Numeric imputation\n",
    "    for col in numeric_cols:\n",
    "        missing_count = df_imputed[col].isnull().sum()\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            if col in medical_features:\n",
    "                # For medical features, use median within similar groups if possible\n",
    "                if target_col and target_col in df.columns:\n",
    "                    # Group by target and use median\n",
    "                    df_imputed[col] = df_imputed.groupby(target_col)[col].transform(\n",
    "                        lambda x: x.fillna(x.median()) if not x.median() != x.median() else x.fillna(df_imputed[col].median())\n",
    "                    )\n",
    "                else:\n",
    "                    df_imputed[col] = df_imputed[col].fillna(df_imputed[col].median())\n",
    "                imputation_method = 'group_median' if target_col else 'median'\n",
    "            else:\n",
    "                # Regular median imputation for other numeric\n",
    "                df_imputed[col] = df_imputed[col].fillna(df_imputed[col].median())\n",
    "                imputation_method = 'median'\n",
    "            \n",
    "            imputation_info[col] = {\n",
    "                'missing_count': missing_count,\n",
    "                'imputation_method': imputation_method\n",
    "            }\n",
    "    \n",
    "    # Categorical imputation\n",
    "    for col in categorical_cols:\n",
    "        missing_count = df_imputed[col].isnull().sum()\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            # Use mode or 'Unknown' if no mode exists\n",
    "            mode_val = df_imputed[col].mode()\n",
    "            if len(mode_val) > 0:\n",
    "                df_imputed[col] = df_imputed[col].fillna(mode_val[0])\n",
    "                imputation_method = 'mode'\n",
    "            else:\n",
    "                df_imputed[col] = df_imputed[col].fillna('Unknown')\n",
    "                imputation_method = 'unknown'\n",
    "            \n",
    "            imputation_info[col] = {\n",
    "                'missing_count': missing_count,\n",
    "                'imputation_method': imputation_method\n",
    "            }\n",
    "    \n",
    "    return df_imputed, imputation_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4caa7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, target_col=None):\n",
    "    \"\"\"Create additional engineered features\"\"\"\n",
    "    df_engineered = df.copy()\n",
    "    new_features = []\n",
    "    \n",
    "    # 1. BMI-related features\n",
    "    if 'bmi' in df.columns:\n",
    "        # BMI risk categories (more detailed)\n",
    "        df_engineered['bmi_risk_level'] = pd.cut(df_engineered['bmi'], \n",
    "                                               bins=[0, 18.5, 25, 30, 35, float('inf')],\n",
    "                                               labels=['underweight', 'normal', 'overweight', 'obese_1', 'obese_2'])\n",
    "        new_features.append('bmi_risk_level')\n",
    "    \n",
    "    # 2. Age-related features\n",
    "    if 'age' in df.columns:\n",
    "        # Age risk for diabetes (medical domain knowledge)\n",
    "        df_engineered['age_diabetes_risk'] = pd.cut(df_engineered['age'],\n",
    "                                                  bins=[0, 35, 45, 65, float('inf')],\n",
    "                                                  labels=['low_risk', 'moderate_risk', 'high_risk', 'very_high_risk'])\n",
    "        new_features.append('age_diabetes_risk')\n",
    "    \n",
    "    # 3. Combined health risk score\n",
    "    health_indicators = ['hypertension', 'heart_disease', 'family_history']\n",
    "    available_indicators = [col for col in health_indicators if col in df.columns]\n",
    "    \n",
    "    if available_indicators:\n",
    "        df_engineered['health_risk_score'] = df_engineered[available_indicators].sum(axis=1)\n",
    "        new_features.append('health_risk_score')\n",
    "    \n",
    "    # 4. Lifestyle score\n",
    "    lifestyle_factors = []\n",
    "    \n",
    "    # Physical activity scoring\n",
    "    if 'physical_activity' in df.columns:\n",
    "        activity_map = {'low': 0, 'moderate': 1, 'high': 2}\n",
    "        df_engineered['activity_score'] = df_engineered['physical_activity'].map(activity_map).fillna(0)\n",
    "        lifestyle_factors.append('activity_score')\n",
    "        new_features.append('activity_score')\n",
    "    \n",
    "    # Sleep quality scoring\n",
    "    if 'sleep_hours' in df.columns:\n",
    "        # Optimal sleep is 7-9 hours\n",
    "        df_engineered['sleep_quality'] = df_engineered['sleep_hours'].apply(\n",
    "            lambda x: 2 if 7 <= x <= 9 else (1 if 6 <= x <= 10 else 0) if pd.notna(x) else 0\n",
    "        )\n",
    "        lifestyle_factors.append('sleep_quality')\n",
    "        new_features.append('sleep_quality')\n",
    "    \n",
    "    # Combined lifestyle score\n",
    "    if lifestyle_factors:\n",
    "        df_engineered['lifestyle_score'] = df_engineered[lifestyle_factors].sum(axis=1)\n",
    "        new_features.append('lifestyle_score')\n",
    "    \n",
    "    # 5. Geographic risk (if environmental_risk is available)\n",
    "    if 'environmental_risk' in df.columns and 'urban_rural' in df.columns:\n",
    "        # Combine environmental risk with urban/rural\n",
    "        urban_risk_map = {'urban': 1.1, 'rural': 0.9}  # Urban areas might have higher risk\n",
    "        df_engineered['location_risk'] = (df_engineered['environmental_risk'] * \n",
    "                                        df_engineered['urban_rural'].map(urban_risk_map).fillna(1.0))\n",
    "        new_features.append('location_risk')\n",
    "    \n",
    "    return df_engineered, new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e16363a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_high_cardinality_categorical(df, categorical_cols, target_col=None, max_categories=10):\n",
    "    \"\"\"Handle high cardinality categorical variables\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    encoding_info = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        unique_count = df_processed[col].nunique()\n",
    "        \n",
    "        if unique_count > max_categories:\n",
    "            # For high cardinality columns like 'location' (states)\n",
    "            if col == 'location':\n",
    "                # Group by frequency - keep top states, others as 'Other'\n",
    "                value_counts = df_processed[col].value_counts()\n",
    "                top_categories = value_counts.head(max_categories).index.tolist()\n",
    "                df_processed[col] = df_processed[col].apply(\n",
    "                    lambda x: x if x in top_categories else 'Other'\n",
    "                )\n",
    "                encoding_info[col] = {\n",
    "                    'method': 'frequency_grouping',\n",
    "                    'kept_categories': len(top_categories) + 1,  # +1 for 'Other'\n",
    "                    'original_categories': unique_count\n",
    "                }\n",
    "            \n",
    "            elif target_col and target_col in df.columns:\n",
    "                # Use target encoding for other high cardinality categorical variables\n",
    "                # This is more sophisticated than frequency grouping\n",
    "                target_encoder = TargetEncoder()\n",
    "                df_processed[f'{col}_target_encoded'] = target_encoder.fit_transform(\n",
    "                    df_processed[[col]], df_processed[target_col]\n",
    "                )\n",
    "                \n",
    "                # Keep original column and add encoded version\n",
    "                encoding_info[col] = {\n",
    "                    'method': 'target_encoding',\n",
    "                    'new_column': f'{col}_target_encoded',\n",
    "                    'original_categories': unique_count\n",
    "                }\n",
    "            else:\n",
    "                # Fallback to frequency grouping\n",
    "                value_counts = df_processed[col].value_counts()\n",
    "                top_categories = value_counts.head(max_categories).index.tolist()\n",
    "                df_processed[col] = df_processed[col].apply(\n",
    "                    lambda x: x if x in top_categories else 'Other'\n",
    "                )\n",
    "                encoding_info[col] = {\n",
    "                    'method': 'frequency_grouping',\n",
    "                    'kept_categories': len(top_categories) + 1,\n",
    "                    'original_categories': unique_count\n",
    "                }\n",
    "    \n",
    "    return df_processed, encoding_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6f90b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_preprocessing(df: pd.DataFrame, outdir: str, target_col: str = None, \n",
    "                         handle_outliers_method='cap', use_feature_engineering=True):\n",
    "    \"\"\"Enhanced preprocessing pipeline with all improvements\"\"\"\n",
    "    ensure_dir(outdir)\n",
    "    \n",
    "    print(\"🔄 Starting Enhanced Preprocessing Pipeline...\")\n",
    "    \n",
    "    # 1. Initial cleaning\n",
    "    print(\"  📋 Step 1: Basic cleaning...\")\n",
    "    df_clean = df.copy()\n",
    "    initial_shape = df_clean.shape\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_clean = df_clean.drop_duplicates().reset_index(drop=True)\n",
    "    print(f\"     Removed {initial_shape[0] - df_clean.shape[0]} duplicate rows\")\n",
    "    \n",
    "    # 2. Identify column types\n",
    "    print(\"  🔍 Step 2: Analyzing column types...\")\n",
    "    numeric_cols = [c for c in df_clean.columns \n",
    "                   if pd.api.types.is_numeric_dtype(df_clean[c]) and c != target_col]\n",
    "    categorical_cols = [c for c in df_clean.columns \n",
    "                       if df_clean[c].dtype == \"object\" and c != target_col]\n",
    "    \n",
    "    # Special handling for 'year' column - treat as categorical\n",
    "    if 'year' in numeric_cols:\n",
    "        print(\"     Moving 'year' from numeric to categorical (ordinal treatment)\")\n",
    "        numeric_cols.remove('year')\n",
    "        categorical_cols.append('year')\n",
    "        # Convert year to string to treat as categorical\n",
    "        df_clean['year'] = df_clean['year'].astype(str)\n",
    "    \n",
    "    # 3. Enhanced imputation\n",
    "    print(\"  🩹 Step 3: Enhanced imputation...\")\n",
    "    df_clean, imputation_info = enhanced_imputation(df_clean, numeric_cols, categorical_cols, target_col)\n",
    "    \n",
    "    # Save imputation info\n",
    "    imputation_df = pd.DataFrame.from_dict(imputation_info, orient='index').reset_index()\n",
    "    imputation_df.columns = ['column', 'missing_count', 'imputation_method']\n",
    "    imputation_df.to_csv(os.path.join(outdir, \"imputation_report.csv\"), index=False)\n",
    "    \n",
    "    # 4. Outlier handling for numeric columns\n",
    "    if numeric_cols and handle_outliers_method != 'none':\n",
    "        print(f\"  🎯 Step 4: Handling outliers using {handle_outliers_method} method...\")\n",
    "        df_clean, outlier_info = handle_outliers(df_clean, numeric_cols, handle_outliers_method)\n",
    "        \n",
    "        # Save outlier info\n",
    "        outlier_df = pd.DataFrame.from_dict(outlier_info, orient='index').reset_index()\n",
    "        outlier_df.to_csv(os.path.join(outdir, \"outlier_treatment_report.csv\"), index=False)\n",
    "    \n",
    "    # 5. Feature Engineering\n",
    "    if use_feature_engineering:\n",
    "        print(\"  ⚙️  Step 5: Feature engineering...\")\n",
    "        df_clean, new_features = feature_engineering(df_clean, target_col)\n",
    "        print(f\"     Created {len(new_features)} new features: {new_features}\")\n",
    "        \n",
    "        # Update column lists with new categorical features\n",
    "        new_categorical = [f for f in new_features if df_clean[f].dtype == 'object']\n",
    "        categorical_cols.extend(new_categorical)\n",
    "        \n",
    "        new_numeric = [f for f in new_features if f not in new_categorical]\n",
    "        numeric_cols.extend(new_numeric)\n",
    "    \n",
    "    # 6. Handle high cardinality categorical variables\n",
    "    print(\"  📊 Step 6: Handling high cardinality categorical variables...\")\n",
    "    df_clean, encoding_info = handle_high_cardinality_categorical(\n",
    "        df_clean, categorical_cols, target_col, max_categories=15\n",
    "    )\n",
    "    \n",
    "    # Save encoding info\n",
    "    if encoding_info:\n",
    "        encoding_df = pd.DataFrame.from_dict(encoding_info, orient='index').reset_index()\n",
    "        encoding_df.to_csv(os.path.join(outdir, \"encoding_report.csv\"), index=False)\n",
    "    \n",
    "    # 7. Save human-readable version\n",
    "    print(\"  💾 Step 7: Saving human-readable version...\")\n",
    "    readable_path = os.path.join(outdir, \"diabetes_enhanced_readable.csv\")\n",
    "    df_clean.to_csv(readable_path, index=False)\n",
    "    \n",
    "    # 8. Prepare ML-ready version\n",
    "    print(\"  🤖 Step 8: Preparing ML-ready version...\")\n",
    "    df_ml = df_clean.copy()\n",
    "    \n",
    "        # Get updated categorical columns (excluding target encoded columns for one-hot encoding)\n",
    "    categorical_for_encoding = [col for col in categorical_cols \n",
    "                               if not any(f'{col}_target_encoded' in colname for colname in df_ml.columns)]\n",
    "    \n",
    "    # One-hot encoding for categorical variables\n",
    "    if categorical_for_encoding:\n",
    "        print(f\"     Applying one-hot encoding to: {categorical_for_encoding}\")\n",
    "        df_ml = pd.get_dummies(df_ml, columns=categorical_for_encoding, drop_first=False)\n",
    "    \n",
    "    # Update numeric columns list (include target encoded features, exclude categorical features)\n",
    "    target_encoded_cols = [col for col in df_ml.columns if 'target_encoded' in col]\n",
    "    \n",
    "    # Filter numeric_cols to only include truly numeric columns that exist in df_ml\n",
    "    final_numeric_cols = []\n",
    "    for col in numeric_cols:\n",
    "        if col in df_ml.columns and pd.api.types.is_numeric_dtype(df_ml[col]):\n",
    "            final_numeric_cols.append(col)\n",
    "    \n",
    "    # Add target encoded columns\n",
    "    final_numeric_cols.extend(target_encoded_cols)\n",
    "    \n",
    "    # 9. Scaling numeric features (excluding year which is now categorical)\n",
    "    if final_numeric_cols:\n",
    "        print(f\"     Scaling {len(final_numeric_cols)} numeric features...\")\n",
    "        scaler = StandardScaler()\n",
    "        df_ml[final_numeric_cols] = scaler.fit_transform(df_ml[final_numeric_cols])\n",
    "        \n",
    "        # Save scaler for later use\n",
    "        import joblib\n",
    "        scaler_path = os.path.join(outdir, \"feature_scaler.pkl\")\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        print(f\"     Scaler saved to: {scaler_path}\")\n",
    "    \n",
    "    # 10. Feature selection (optional - select top K features for numeric columns only)\n",
    "    if target_col and target_col in df_ml.columns and len(df_ml.columns) > 50:\n",
    "        print(\"  🎯 Step 9: Feature selection (too many features detected)...\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = df_ml.drop(columns=[target_col])\n",
    "        y = df_ml[target_col]\n",
    "        \n",
    "        # Only apply feature selection to numeric columns\n",
    "        numeric_feature_cols = [col for col in X.columns if pd.api.types.is_numeric_dtype(X[col])]\n",
    "        categorical_feature_cols = [col for col in X.columns if not pd.api.types.is_numeric_dtype(X[col])]\n",
    "        \n",
    "        if numeric_feature_cols and len(numeric_feature_cols) > 30:\n",
    "            # Select top K numeric features\n",
    "            k = min(20, len(numeric_feature_cols))  # Select top 20 numeric or all available\n",
    "            selector = SelectKBest(score_func=f_classif, k=k)\n",
    "            X_numeric_selected = selector.fit_transform(X[numeric_feature_cols], y)\n",
    "            \n",
    "            # Get selected feature names\n",
    "            selected_numeric_features = pd.Series(numeric_feature_cols)[selector.get_support()].tolist()\n",
    "            \n",
    "            # Combine selected numeric features with all categorical features and target\n",
    "            selected_features = selected_numeric_features + categorical_feature_cols + [target_col]\n",
    "            \n",
    "            df_ml = df_ml[selected_features]\n",
    "            \n",
    "            # Save feature selection info\n",
    "            feature_scores = pd.DataFrame({\n",
    "                'feature': numeric_feature_cols,\n",
    "                'score': selector.scores_,\n",
    "                'selected': selector.get_support()\n",
    "            }).sort_values('score', ascending=False)\n",
    "            \n",
    "            feature_scores.to_csv(os.path.join(outdir, \"feature_selection_report.csv\"), index=False)\n",
    "            print(f\"     Selected {k} numeric features out of {len(numeric_feature_cols)} (kept all {len(categorical_feature_cols)} categorical features)\")\n",
    "        else:\n",
    "            print(\"     Skipping feature selection - not enough numeric features or features already manageable\")\n",
    "    \n",
    "    # 11. Save ML-ready version\n",
    "    print(\"  💾 Step 10: Saving ML-ready version...\")\n",
    "    ml_path = os.path.join(outdir, \"diabetes_enhanced_ml_ready.csv\")\n",
    "    df_ml.to_csv(ml_path, index=False)\n",
    "    \n",
    "    # 12. Generate processing summary\n",
    "    processing_summary = {\n",
    "        'original_rows': initial_shape[0],\n",
    "        'original_columns': initial_shape[1],\n",
    "        'final_rows': df_ml.shape[0],\n",
    "        'final_columns': df_ml.shape[1],\n",
    "        'duplicates_removed': initial_shape[0] - df_clean.shape[0],\n",
    "        'numeric_features': len([c for c in final_numeric_cols if c in df_ml.columns]),\n",
    "        'categorical_features_encoded': len(categorical_for_encoding),\n",
    "        'new_features_created': len(new_features) if use_feature_engineering else 0,\n",
    "        'outlier_method': handle_outliers_method,\n",
    "        'feature_selection_applied': 'Yes' if len(df_ml.columns) != len(df_clean.columns) else 'No',\n",
    "        'total_final_features': len(df_ml.columns) - (1 if target_col in df_ml.columns else 0)\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame.from_dict(processing_summary, orient='index', columns=['value'])\n",
    "    summary_df.to_csv(os.path.join(outdir, \"processing_summary.csv\"))\n",
    "    \n",
    "    print(f\"✅ Enhanced preprocessing completed!\")\n",
    "    print(f\"   📁 Human-readable data: {readable_path}\")\n",
    "    print(f\"   🤖 ML-ready data: {ml_path}\")\n",
    "    print(f\"   📊 Final dataset: {df_ml.shape[0]} rows × {df_ml.shape[1]} columns\")\n",
    "    \n",
    "    return readable_path, ml_path, processing_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bc4e701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading dataset from: ../data/raw/diabetes_dataset_E.csv\n",
      "   Dataset loaded: 100000 rows × 28 columns\n",
      "🎯 Auto-detected target column: diabetes\n",
      "\n",
      "🔍 Running comprehensive EDA...\n",
      "🔍 Running Comprehensive EDA...\n",
      "   Dataset loaded: 100000 rows × 28 columns\n",
      "🎯 Auto-detected target column: diabetes\n",
      "\n",
      "🔍 Running comprehensive EDA...\n",
      "🔍 Running Comprehensive EDA...\n",
      "✅ Enhanced EDA completed. Reports saved to: ../data/reports_enhanced\n",
      "EDA finished. Reports saved to: ../data/reports_enhanced\n",
      "\n",
      "🔄 Running enhanced preprocessing...\n",
      "🔄 Starting Enhanced Preprocessing Pipeline...\n",
      "  📋 Step 1: Basic cleaning...\n",
      "✅ Enhanced EDA completed. Reports saved to: ../data/reports_enhanced\n",
      "EDA finished. Reports saved to: ../data/reports_enhanced\n",
      "\n",
      "🔄 Running enhanced preprocessing...\n",
      "🔄 Starting Enhanced Preprocessing Pipeline...\n",
      "  📋 Step 1: Basic cleaning...\n",
      "     Removed 0 duplicate rows\n",
      "  🔍 Step 2: Analyzing column types...\n",
      "     Moving 'year' from numeric to categorical (ordinal treatment)\n",
      "  🩹 Step 3: Enhanced imputation...\n",
      "     Removed 0 duplicate rows\n",
      "  🔍 Step 2: Analyzing column types...\n",
      "     Moving 'year' from numeric to categorical (ordinal treatment)\n",
      "  🩹 Step 3: Enhanced imputation...\n",
      "  🎯 Step 4: Handling outliers using cap method...\n",
      "  🎯 Step 4: Handling outliers using cap method...\n",
      "  ⚙️  Step 5: Feature engineering...\n",
      "  ⚙️  Step 5: Feature engineering...\n",
      "     Created 7 new features: ['bmi_risk_level', 'age_diabetes_risk', 'health_risk_score', 'activity_score', 'sleep_quality', 'lifestyle_score', 'location_risk']\n",
      "  📊 Step 6: Handling high cardinality categorical variables...\n",
      "     Created 7 new features: ['bmi_risk_level', 'age_diabetes_risk', 'health_risk_score', 'activity_score', 'sleep_quality', 'lifestyle_score', 'location_risk']\n",
      "  📊 Step 6: Handling high cardinality categorical variables...\n",
      "  💾 Step 7: Saving human-readable version...\n",
      "  💾 Step 7: Saving human-readable version...\n",
      "  🤖 Step 8: Preparing ML-ready version...\n",
      "     Applying one-hot encoding to: ['gender', 'location', 'smoking_history', 'bmi_category', 'age_group', 'physical_activity', 'diet_pattern', 'alcohol_intake', 'urban_rural', 'region_income', 'year']\n",
      "  🤖 Step 8: Preparing ML-ready version...\n",
      "     Applying one-hot encoding to: ['gender', 'location', 'smoking_history', 'bmi_category', 'age_group', 'physical_activity', 'diet_pattern', 'alcohol_intake', 'urban_rural', 'region_income', 'year']\n",
      "     Scaling 21 numeric features...\n",
      "     Scaler saved to: ../data/processed_enhanced\\feature_scaler.pkl\n",
      "  🎯 Step 9: Feature selection (too many features detected)...\n",
      "     Scaling 21 numeric features...\n",
      "     Scaler saved to: ../data/processed_enhanced\\feature_scaler.pkl\n",
      "  🎯 Step 9: Feature selection (too many features detected)...\n",
      "     Selected 20 numeric features out of 75 (kept all 2 categorical features)\n",
      "  💾 Step 10: Saving ML-ready version...\n",
      "     Selected 20 numeric features out of 75 (kept all 2 categorical features)\n",
      "  💾 Step 10: Saving ML-ready version...\n",
      "✅ Enhanced preprocessing completed!\n",
      "   📁 Human-readable data: ../data/processed_enhanced\\diabetes_enhanced_readable.csv\n",
      "   🤖 ML-ready data: ../data/processed_enhanced\\diabetes_enhanced_ml_ready.csv\n",
      "   📊 Final dataset: 100000 rows × 23 columns\n",
      "Preprocessing finished. Outputs:\n",
      " - Human-readable: ../data/processed_enhanced\\diabetes_enhanced_readable.csv\n",
      " - ML-ready: ../data/processed_enhanced\\diabetes_enhanced_ml_ready.csv\n",
      " - Summary: {'original_rows': 100000, 'original_columns': 28, 'final_rows': 100000, 'final_columns': 23, 'duplicates_removed': 0, 'numeric_features': 4, 'categorical_features_encoded': 11, 'new_features_created': 7, 'outlier_method': 'cap', 'feature_selection_applied': 'Yes', 'total_final_features': 22}\n",
      "\n",
      "✅ Interactive runner cell complete. Toggle `run_eda` / `run_preprocessing` and re-run to execute steps.\n",
      "✅ Enhanced preprocessing completed!\n",
      "   📁 Human-readable data: ../data/processed_enhanced\\diabetes_enhanced_readable.csv\n",
      "   🤖 ML-ready data: ../data/processed_enhanced\\diabetes_enhanced_ml_ready.csv\n",
      "   📊 Final dataset: 100000 rows × 23 columns\n",
      "Preprocessing finished. Outputs:\n",
      " - Human-readable: ../data/processed_enhanced\\diabetes_enhanced_readable.csv\n",
      " - ML-ready: ../data/processed_enhanced\\diabetes_enhanced_ml_ready.csv\n",
      " - Summary: {'original_rows': 100000, 'original_columns': 28, 'final_rows': 100000, 'final_columns': 23, 'duplicates_removed': 0, 'numeric_features': 4, 'categorical_features_encoded': 11, 'new_features_created': 7, 'outlier_method': 'cap', 'feature_selection_applied': 'Yes', 'total_final_features': 22}\n",
      "\n",
      "✅ Interactive runner cell complete. Toggle `run_eda` / `run_preprocessing` and re-run to execute steps.\n"
     ]
    }
   ],
   "source": [
    "# Interactive runner (notebook-friendly)\n",
    "# This cell replaces the script-style `main()` function. Execute to load data and\n",
    "# optionally run EDA and preprocessing steps interactively.\n",
    "\n",
    "# --- Configuration (edit paths as needed) ---\n",
    "raw_path = \"../data/raw/diabetes_dataset_E.csv\"  # relative to this notebook\n",
    "reports_dir = \"../data/reports_enhanced\"\n",
    "outdir = \"../data/processed_enhanced\"\n",
    "\n",
    "# Safety toggles - set True to run the step when you execute this cell\n",
    "run_eda = False\n",
    "run_preprocessing = False\n",
    "\n",
    "# Load dataset\n",
    "print(f\"📂 Loading dataset from: {raw_path}\")\n",
    "df = pd.read_csv(raw_path)\n",
    "print(f\"   Dataset loaded: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "run_eda = True  # Enable EDA by default for interactive exploration\n",
    "run_preprocessing = True  # Enable preprocessing by default for interactive exploration\n",
    "# Auto-detect target (if any)\n",
    "target_col = guess_target(df)\n",
    "if target_col:\n",
    "    print(f\"🎯 Auto-detected target column: {target_col}\")\n",
    "else:\n",
    "    print(\"⚠️  No target column auto-detected; pass `target_col` explicitly to functions if needed\")\n",
    "\n",
    "# Run EDA if requested\n",
    "if run_eda:\n",
    "    print(\"\\n🔍 Running comprehensive EDA...\")\n",
    "    ensure_dir(reports_dir)\n",
    "    basic_info = comprehensive_eda(df, reports_dir=reports_dir, target_col=target_col)\n",
    "    print(\"EDA finished. Reports saved to:\", reports_dir)\n",
    "\n",
    "# Run preprocessing if requested\n",
    "if run_preprocessing:\n",
    "    print(\"\\n🔄 Running enhanced preprocessing...\")\n",
    "    ensure_dir(outdir)\n",
    "    readable_path, ml_path, summary = enhanced_preprocessing(\n",
    "        df,\n",
    "        outdir=outdir,\n",
    "        target_col=target_col,\n",
    "        handle_outliers_method='cap',\n",
    "        use_feature_engineering=True,\n",
    "    )\n",
    "    print(\"Preprocessing finished. Outputs:\")\n",
    "    print(\" - Human-readable:\", readable_path)\n",
    "    print(\" - ML-ready:\", ml_path)\n",
    "    print(\" - Summary:\", summary)\n",
    "\n",
    "print(\"\\n✅ Interactive runner cell complete. Toggle `run_eda` / `run_preprocessing` and re-run to execute steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2973d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ML-ready data: (100000, 23)\n",
      "Using target column: diabetes\n",
      "One-hot encoding leftover object columns: ['bmi_risk_level', 'age_diabetes_risk']\n",
      "Train shape: (80000, 29) Test shape: (20000, 29)\n",
      "Running RandomizedSearchCV (25 iterations) to reduce overfitting risk...\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Best params: {'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 0.5, 'max_depth': 10, 'bootstrap': True}\n",
      "Best params: {'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 0.5, 'max_depth': 10, 'bootstrap': True}\n",
      "Train CV ROC-AUC: mean=0.9760 std=0.0013\n",
      "Train CV ROC-AUC: mean=0.9760 std=0.0013\n",
      "Threshold: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94     18300\n",
      "           1       0.46      0.90      0.61      1700\n",
      "\n",
      "    accuracy                           0.90     20000\n",
      "   macro avg       0.72      0.90      0.77     20000\n",
      "weighted avg       0.94      0.90      0.91     20000\n",
      "\n",
      "Threshold: 0.4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92     18300\n",
      "           1       0.38      0.94      0.54      1700\n",
      "\n",
      "    accuracy                           0.86     20000\n",
      "   macro avg       0.69      0.90      0.73     20000\n",
      "weighted avg       0.94      0.86      0.89     20000\n",
      "\n",
      "Threshold: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90     18300\n",
      "           1       0.34      0.97      0.50      1700\n",
      "\n",
      "    accuracy                           0.83     20000\n",
      "   macro avg       0.67      0.89      0.70     20000\n",
      "weighted avg       0.94      0.83      0.87     20000\n",
      "\n",
      "RF Test ROC-AUC: 0.9748, RF Test PR-AUC: 0.8717\n",
      "Classification report (RF test set):\n",
      "Threshold: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94     18300\n",
      "           1       0.46      0.90      0.61      1700\n",
      "\n",
      "    accuracy                           0.90     20000\n",
      "   macro avg       0.72      0.90      0.77     20000\n",
      "weighted avg       0.94      0.90      0.91     20000\n",
      "\n",
      "Threshold: 0.4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92     18300\n",
      "           1       0.38      0.94      0.54      1700\n",
      "\n",
      "    accuracy                           0.86     20000\n",
      "   macro avg       0.69      0.90      0.73     20000\n",
      "weighted avg       0.94      0.86      0.89     20000\n",
      "\n",
      "Threshold: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90     18300\n",
      "           1       0.34      0.97      0.50      1700\n",
      "\n",
      "    accuracy                           0.83     20000\n",
      "   macro avg       0.67      0.89      0.70     20000\n",
      "weighted avg       0.94      0.83      0.87     20000\n",
      "\n",
      "RF Test ROC-AUC: 0.9748, RF Test PR-AUC: 0.8717\n",
      "Classification report (RF test set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94     18300\n",
      "           1       0.46      0.90      0.61      1700\n",
      "\n",
      "    accuracy                           0.90     20000\n",
      "   macro avg       0.72      0.90      0.77     20000\n",
      "weighted avg       0.94      0.90      0.91     20000\n",
      "\n",
      "CV mean - Test ROC difference (RF): 0.0012 (positive → possible overfitting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94     18300\n",
      "           1       0.46      0.90      0.61      1700\n",
      "\n",
      "    accuracy                           0.90     20000\n",
      "   macro avg       0.72      0.90      0.77     20000\n",
      "weighted avg       0.94      0.90      0.91     20000\n",
      "\n",
      "CV mean - Test ROC difference (RF): 0.0012 (positive → possible overfitting)\n",
      "Saved tuned RF model to: ../models\\diabetes_rf_tuned.pkl\n",
      "Saved RF report and feature importances in ../models\n",
      "Saved tuned RF model to: ../models\\diabetes_rf_tuned.pkl\n",
      "Saved RF report and feature importances in ../models\n",
      "Training Keras MLP with BatchNorm, Dropout and EarlyStopping...\n",
      "Training Keras MLP with BatchNorm, Dropout and EarlyStopping...\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - auc: 0.9041 - loss: 0.1939 - val_auc: 0.9671 - val_loss: 0.1098\n",
      "Epoch 2/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - auc: 0.9041 - loss: 0.1939 - val_auc: 0.9671 - val_loss: 0.1098\n",
      "Epoch 2/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - auc: 0.9531 - loss: 0.1198 - val_auc: 0.9710 - val_loss: 0.1006\n",
      "Epoch 3/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - auc: 0.9531 - loss: 0.1198 - val_auc: 0.9710 - val_loss: 0.1006\n",
      "Epoch 3/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - auc: 0.9599 - loss: 0.1100 - val_auc: 0.9710 - val_loss: 0.1011\n",
      "Epoch 4/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - auc: 0.9599 - loss: 0.1100 - val_auc: 0.9710 - val_loss: 0.1011\n",
      "Epoch 4/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - auc: 0.9623 - loss: 0.1059 - val_auc: 0.9712 - val_loss: 0.0991\n",
      "Epoch 5/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - auc: 0.9623 - loss: 0.1059 - val_auc: 0.9712 - val_loss: 0.0991\n",
      "Epoch 5/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - auc: 0.9649 - loss: 0.1016 - val_auc: 0.9717 - val_loss: 0.0966\n",
      "Epoch 6/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - auc: 0.9649 - loss: 0.1016 - val_auc: 0.9717 - val_loss: 0.0966\n",
      "Epoch 6/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - auc: 0.9654 - loss: 0.1000 - val_auc: 0.9723 - val_loss: 0.0954\n",
      "Epoch 7/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - auc: 0.9654 - loss: 0.1000 - val_auc: 0.9723 - val_loss: 0.0954\n",
      "Epoch 7/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - auc: 0.9659 - loss: 0.0992 - val_auc: 0.9725 - val_loss: 0.0950\n",
      "Epoch 8/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - auc: 0.9659 - loss: 0.0992 - val_auc: 0.9725 - val_loss: 0.0950\n",
      "Epoch 8/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - auc: 0.9669 - loss: 0.0976 - val_auc: 0.9725 - val_loss: 0.0929\n",
      "Epoch 9/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - auc: 0.9669 - loss: 0.0976 - val_auc: 0.9725 - val_loss: 0.0929\n",
      "Epoch 9/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - auc: 0.9682 - loss: 0.0955 - val_auc: 0.9719 - val_loss: 0.0952\n",
      "Epoch 10/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - auc: 0.9682 - loss: 0.0955 - val_auc: 0.9719 - val_loss: 0.0952\n",
      "Epoch 10/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - auc: 0.9689 - loss: 0.0947 - val_auc: 0.9719 - val_loss: 0.0932\n",
      "Epoch 11/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - auc: 0.9689 - loss: 0.0947 - val_auc: 0.9719 - val_loss: 0.0932\n",
      "Epoch 11/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - auc: 0.9686 - loss: 0.0951 - val_auc: 0.9732 - val_loss: 0.0930\n",
      "Epoch 12/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - auc: 0.9686 - loss: 0.0951 - val_auc: 0.9732 - val_loss: 0.0930\n",
      "Epoch 12/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - auc: 0.9697 - loss: 0.0939 - val_auc: 0.9725 - val_loss: 0.0906\n",
      "Epoch 13/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - auc: 0.9697 - loss: 0.0939 - val_auc: 0.9725 - val_loss: 0.0906\n",
      "Epoch 13/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - auc: 0.9702 - loss: 0.0932 - val_auc: 0.9728 - val_loss: 0.0912\n",
      "Epoch 14/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - auc: 0.9702 - loss: 0.0932 - val_auc: 0.9728 - val_loss: 0.0912\n",
      "Epoch 14/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - auc: 0.9695 - loss: 0.0929 - val_auc: 0.9717 - val_loss: 0.0935\n",
      "Epoch 15/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - auc: 0.9695 - loss: 0.0929 - val_auc: 0.9717 - val_loss: 0.0935\n",
      "Epoch 15/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - auc: 0.9703 - loss: 0.0924 - val_auc: 0.9730 - val_loss: 0.0916\n",
      "Epoch 16/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - auc: 0.9703 - loss: 0.0924 - val_auc: 0.9730 - val_loss: 0.0916\n",
      "Epoch 16/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - auc: 0.9705 - loss: 0.0920 - val_auc: 0.9726 - val_loss: 0.0909\n",
      "Epoch 17/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - auc: 0.9705 - loss: 0.0920 - val_auc: 0.9726 - val_loss: 0.0909\n",
      "Epoch 17/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - auc: 0.9712 - loss: 0.0919 - val_auc: 0.9712 - val_loss: 0.0924\n",
      "Epoch 18/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - auc: 0.9712 - loss: 0.0919 - val_auc: 0.9712 - val_loss: 0.0924\n",
      "Epoch 18/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - auc: 0.9715 - loss: 0.0906 - val_auc: 0.9715 - val_loss: 0.0950\n",
      "Epoch 19/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - auc: 0.9715 - loss: 0.0906 - val_auc: 0.9715 - val_loss: 0.0950\n",
      "Epoch 19/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - auc: 0.9714 - loss: 0.0903 - val_auc: 0.9726 - val_loss: 0.0897\n",
      "Epoch 20/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - auc: 0.9714 - loss: 0.0903 - val_auc: 0.9726 - val_loss: 0.0897\n",
      "Epoch 20/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - auc: 0.9714 - loss: 0.0898 - val_auc: 0.9731 - val_loss: 0.0890\n",
      "Epoch 21/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - auc: 0.9714 - loss: 0.0898 - val_auc: 0.9731 - val_loss: 0.0890\n",
      "Epoch 21/100\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - auc: 0.9719 - loss: 0.0897 - val_auc: 0.9728 - val_loss: 0.0889\n",
      "Epoch 21: early stopping\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - auc: 0.9719 - loss: 0.0897 - val_auc: 0.9728 - val_loss: 0.0889\n",
      "Epoch 21: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Test ROC-AUC: 0.9738, NN Test PR-AUC: 0.8667\n",
      "Best Val AUC - Test ROC difference (NN): -0.0006 (positive → possible overfitting)\n",
      "Saved NN model to: ../models\\diabetes_nn_dropout_batchnorm.h5\n",
      "Chosen model based on test ROC: rf (rf=0.9748, nn=0.9738)\n",
      "Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# --- RandomForest baseline + hyperparameter tuning (RandomizedSearchCV) ---\n",
    "# Runs a randomized hyperparameter search, compares CV performance vs test to detect overfitting,\n",
    "# and optionally trains a simple Keras MLP with BatchNormalization, Dropout and EarlyStopping\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Optional: neural net tools (try/except so notebook still works if TF not installed)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, BatchNormalization\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    from sklearn.metrics import make_scorer, recall_score\n",
    "    TF_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    TF_AVAILABLE = False\n",
    "\n",
    "# Paths (edit if needed)\n",
    "ml_path = \"../data/processed_enhanced/diabetes_enhanced_ml_ready.csv\"\n",
    "models_dir = \"../models\"\n",
    "ensure_dir(models_dir)\n",
    "\n",
    "if not os.path.exists(ml_path):\n",
    "    print(f\"ML-ready CSV not found at: {ml_path}. Run preprocessing first or update `ml_path`.\")\n",
    "else:\n",
    "    df_ml = pd.read_csv(ml_path)\n",
    "    print(f\"Loaded ML-ready data: {df_ml.shape}\")\n",
    "\n",
    "    # Detect target\n",
    "    target_col = guess_target(df_ml)\n",
    "    if not target_col:\n",
    "        raise ValueError(\"No target column found in ML-ready CSV. Please supply `target_col` or ensure preprocessing kept the target column.\")\n",
    "    print(f\"Using target column: {target_col}\")\n",
    "\n",
    "    X = df_ml.drop(columns=[target_col])\n",
    "    y = df_ml[target_col]\n",
    "    \n",
    "    # One-hot encode object columns if any remain\n",
    "    obj_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    if obj_cols:\n",
    "        print('One-hot encoding leftover object columns:', obj_cols)\n",
    "        X = pd.get_dummies(X, columns=obj_cols, drop_first=False)\n",
    "\n",
    "    # Simple NaN check\n",
    "    if X.isnull().any().any():\n",
    "        raise ValueError('ML features contain NaNs. Please fix preprocessing before training.')\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n",
    "\n",
    "    # Baseline estimator (RandomForest with RandomizedSearchCV)\n",
    "    base_clf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "\n",
    "    # Parameter distributions for RandomizedSearch\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 400, 800],\n",
    "        'max_depth': [None, 5, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4, 8],\n",
    "        'max_features': ['sqrt', 'log2', 0.2, 0.5],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    scorer = make_scorer(recall_score, pos_label=1)\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=base_clf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=25,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        cv=cv,\n",
    "        random_state=42,\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    print('Running RandomizedSearchCV (25 iterations) to reduce overfitting risk...')\n",
    "    rs.fit(X_train, y_train)\n",
    "\n",
    "    print('Best params:', rs.best_params_)\n",
    "    best = rs.best_estimator_\n",
    "\n",
    "    # Cross-validated performance of best estimator on training folds\n",
    "    train_cv_scores = cross_val_score(best, X_train, y_train, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    print(f\"Train CV ROC-AUC: mean={train_cv_scores.mean():.4f} std={train_cv_scores.std():.4f}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    if hasattr(best, 'predict_proba'):\n",
    "        y_prob = best.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_prob = best.predict(X_test)\n",
    "#################################################################\n",
    "    for thresh in [0.5, 0.4, 0.3]:\n",
    "        print (f\"Threshold: {thresh}\")\n",
    "        y_pred_thresh = (y_prob >= thresh).astype(int)\n",
    "        print(classification_report(y_test, y_pred_thresh))\n",
    "\n",
    "    test_roc = roc_auc_score(y_test, y_prob)\n",
    "    test_pr = average_precision_score(y_test, y_prob)\n",
    "    print(f\"RF Test ROC-AUC: {test_roc:.4f}, RF Test PR-AUC: {test_pr:.4f}\")\n",
    "    print('Classification report (RF test set):')\n",
    "    print(classification_report(y_test, best.predict(X_test)))\n",
    "\n",
    "    # Overfitting check: compare training CV mean vs test\n",
    "    delta = train_cv_scores.mean() - test_roc\n",
    "    print(f\"CV mean - Test ROC difference (RF): {delta:.4f} (positive → possible overfitting)\")\n",
    "\n",
    "    # Save RF best model and artifacts\n",
    "    rf_model_path = os.path.join(models_dir, 'diabetes_rf_tuned.pkl')\n",
    "    joblib.dump(best, rf_model_path)\n",
    "\n",
    "    manifest = {'features': X.columns.tolist(), 'target': target_col}\n",
    "    with open(os.path.join(models_dir, 'feature_columns.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "\n",
    "    report = {\n",
    "        'rf_best_params': rs.best_params_,\n",
    "        'rf_train_cv_mean': float(train_cv_scores.mean()),\n",
    "        'rf_train_cv_std': float(train_cv_scores.std()),\n",
    "        'rf_test_roc_auc': float(test_roc),\n",
    "        'rf_test_pr_auc': float(test_pr),\n",
    "        'rf_model_path': rf_model_path,\n",
    "        'ml_path': ml_path\n",
    "    }\n",
    "    pd.DataFrame([report]).to_csv(os.path.join(models_dir, 'training_report.csv'), index=False)\n",
    "\n",
    "    # Feature importances\n",
    "    importances = pd.Series(best.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "    importances.head(50).to_csv(os.path.join(models_dir, 'feature_importances_top50.csv'))\n",
    "\n",
    "    print('Saved tuned RF model to:', rf_model_path)\n",
    "    print('Saved RF report and feature importances in', models_dir)\n",
    "\n",
    "    # Quick suggestions based on RF results\n",
    "    if delta > 0.05:\n",
    "        print('\\n⚠️  Significant drop from CV → test for RF (delta > 0.05). Suggestions:')\n",
    "        print(' - Reduce model complexity (lower max_depth, increase min_samples_leaf).')\n",
    "        print(' - Use stronger regularization or fewer features (SelectFromModel, Drop low-importance features).')\n",
    "        print(' - Verify no data leakage and that preprocessing didn\\'t use target information.')\n",
    "        print(' - Try gradient boosting with early stopping (XGBoost/LightGBM/CatBoost) and proper validation folds.')\n",
    "\n",
    "    # --- Optional: Train a small Keras MLP with BatchNorm, Dropout and EarlyStopping to compare ---\n",
    "    run_nn = True  # Set to False to skip NN training\n",
    "    if run_nn:\n",
    "        if not TF_AVAILABLE:\n",
    "            print('TensorFlow not available. Skipping NN training. Install tensorflow to enable this option.')\n",
    "        else:\n",
    "            # Ensure reproducibility\n",
    "            tf.random.set_seed(42)\n",
    "            np.random.seed(42)\n",
    "\n",
    "            # Convert to numpy arrays (TF expects floats)\n",
    "            Xtr = X_train.values.astype('float32')\n",
    "            Xte = X_test.values.astype('float32')\n",
    "            ytr = y_train.values.astype('float32')\n",
    "            yte = y_test.values.astype('float32')\n",
    "\n",
    "            input_dim = Xtr.shape[1]\n",
    "\n",
    "            def build_mlp(input_dim, dropout_rate=0.4):\n",
    "                model = Sequential()\n",
    "                model.add(Dense(256, activation='relu', input_shape=(input_dim,)))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(dropout_rate))\n",
    "                model.add(Dense(128, activation='relu'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(dropout_rate))\n",
    "                model.add(Dense(64, activation='relu'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(dropout_rate))\n",
    "                model.add(Dense(1, activation='sigmoid'))\n",
    "                model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "                return model\n",
    "\n",
    "            print('Training Keras MLP with BatchNorm, Dropout and EarlyStopping...')\n",
    "            mlp = build_mlp(input_dim, dropout_rate=0.4)\n",
    "            es = EarlyStopping(monitor='val_auc', mode='max', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "            history = mlp.fit(\n",
    "                Xtr, ytr,\n",
    "                validation_split=0.2,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                callbacks=[es],\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            # Evaluate NN on test set\n",
    "            y_prob_nn = mlp.predict(Xte).ravel()\n",
    "            test_roc_nn = roc_auc_score(yte, y_prob_nn)\n",
    "            test_pr_nn = average_precision_score(yte, y_prob_nn)\n",
    "            print(f\"NN Test ROC-AUC: {test_roc_nn:.4f}, NN Test PR-AUC: {test_pr_nn:.4f}\")\n",
    "\n",
    "            # Overfitting check for NN: best validation AUC vs test AUC\n",
    "            val_auc_hist = history.history.get('val_auc', [])\n",
    "            best_val_auc = max(val_auc_hist) if val_auc_hist else None\n",
    "            if best_val_auc is not None:\n",
    "                delta_nn = best_val_auc - test_roc_nn\n",
    "                print(f\"Best Val AUC - Test ROC difference (NN): {delta_nn:.4f} (positive → possible overfitting)\")\n",
    "            else:\n",
    "                delta_nn = None\n",
    "\n",
    "            # Save NN model and report if desirable\n",
    "            nn_model_path = os.path.join(models_dir, 'diabetes_nn_dropout_batchnorm.h5')\n",
    "            mlp.save(nn_model_path)\n",
    "            print('Saved NN model to:', nn_model_path)\n",
    "\n",
    "            # Compare RF vs NN on test ROC and choose winner\n",
    "            chosen = 'rf' if test_roc >= test_roc_nn else 'nn'\n",
    "            print(f'Chosen model based on test ROC: {chosen} (rf={test_roc:.4f}, nn={test_roc_nn:.4f})')\n",
    "\n",
    "            # Append NN info to report file\n",
    "            extra = {\n",
    "                'nn_test_roc_auc': float(test_roc_nn),\n",
    "                'nn_test_pr_auc': float(test_pr_nn),\n",
    "                'nn_model_path': nn_model_path,\n",
    "                'nn_best_val_auc': float(best_val_auc) if best_val_auc is not None else None,\n",
    "                'nn_val_minus_test': float(delta_nn) if delta_nn is not None else None,\n",
    "                'chosen_model': chosen\n",
    "            }\n",
    "            # Merge and save full report\n",
    "            report.update(extra)\n",
    "            pd.DataFrame([report]).to_csv(os.path.join(models_dir, 'training_report.csv'), index=False)\n",
    "\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f792c479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'bmi', 'hbA1c_level', 'blood_glucose_level', 'gender_Female',\n",
      "       'gender_Male', 'smoking_history_No Info', 'smoking_history_current',\n",
      "       'smoking_history_ever', 'smoking_history_former',\n",
      "       'smoking_history_never', 'smoking_history_not current',\n",
      "       'bmi_category_Normal', 'bmi_category_Obese', 'bmi_category_Overweight',\n",
      "       'bmi_category_Underweight', 'age_group_Adult', 'age_group_Child',\n",
      "       'age_group_Middle-aged', 'age_group_Senior', 'bmi_risk_level_normal',\n",
      "       'bmi_risk_level_obese_1', 'bmi_risk_level_obese_2',\n",
      "       'bmi_risk_level_overweight', 'bmi_risk_level_underweight',\n",
      "       'age_diabetes_risk_high_risk', 'age_diabetes_risk_low_risk',\n",
      "       'age_diabetes_risk_moderate_risk', 'age_diabetes_risk_very_high_risk'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print (X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfe80adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.50: Precision=0.456, Recall=0.901\n",
      "Threshold 0.40: Precision=0.378, Recall=0.936\n",
      "Threshold 0.30: Precision=0.335, Recall=0.965\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, precision_score\n",
    "\n",
    "# Get probabilities for positive class\n",
    "y_proba = best.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute PR curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "ap_score = average_precision_score(y_test, y_proba)\n",
    "\n",
    "# Plot Precision–Recall vs Threshold\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(thresholds, precision[:-1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recall[:-1], \"g-\", label=\"Recall\")\n",
    "plt.xlabel(\"Decision Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(f\"Precision-Recall vs Threshold (AP = {ap_score:.3f})\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print values at common thresholds\n",
    "for t in [0.5, 0.4, 0.3]:\n",
    "    \n",
    "    y_pred = (y_proba >= t).astype(int)\n",
    "    p = precision_score(y_test, y_pred)\n",
    "    r = recall_score(y_test, y_pred)\n",
    "    print(f\"Threshold {t:.2f}: Precision={p:.3f}, Recall={r:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2ce26a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n",
      "Predicted probability (class 0, class 1): [0.00985749 0.99014251]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load model\n",
    "rf_model = joblib.load(\"../models/diabetes_rf_tuned.pkl\")\n",
    "\n",
    "# 1️⃣ Numeric features\n",
    "numeric_features = [\n",
    "    0.8,    # age (normalized)\n",
    "    1.2,    # bmi (normalized)\n",
    "    1.5,    # hbA1c_level (high → indicates diabetes)\n",
    "    1.3     # blood_glucose_level (high)\n",
    "]\n",
    "\n",
    "# Boolean / one-hot features (in exact training order)\n",
    "boolean_features = [\n",
    "    0,1,        # gender_Female, gender_Male\n",
    "    0,0,1,0,0,0, # smoking_history_* (current smoker)\n",
    "    0,0,1,0,    # bmi_category_* (Overweight)\n",
    "    0,0,0,1,    # age_group_* (Senior)\n",
    "    0,0,0,1,0,  # bmi_risk_level_* (overweight risk)\n",
    "    0,0,1,0     # age_diabetes_risk_* (high_risk)\n",
    "]\n",
    "\n",
    "# ✅ Combine numeric + boolean features\n",
    "sample_input = numeric_features + boolean_features\n",
    "\n",
    "# Convert to 2D array\n",
    "sample_input = np.array(sample_input).reshape(1, -1)\n",
    "\n",
    "# 3️⃣ Make prediction\n",
    "pred_class = rf_model.predict(sample_input)\n",
    "pred_proba = rf_model.predict_proba(sample_input)\n",
    "\n",
    "print(\"Predicted class:\", pred_class[0])\n",
    "print(\"Predicted probability (class 0, class 1):\", pred_proba[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
